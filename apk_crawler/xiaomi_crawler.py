import urllib
import requests
import re
from bs4 import BeautifulSoup
import hashlib
import utils
import config
import os,sys

def parser_apks(cate,pages):
    _root_url = "http://app.mi.com"

    for page_num in range(40,50):

        res_parser = {}

        try_times = 0
        while True:
            try:
                print("开始爬取第" + str(cate) + "类 第" + str(page_num) + "页")
                wbdata = requests.get("http://app.mi.com/catTopList/"+str(cate)+"?page=" + str(page_num),timeout=5).text
                print("爬取成功")
                break
            except Exception as e:
                try_times += 1
                print("Error: ",e," 第"+str(try_times)+"次重新尝试")
                if try_times > 4:
                    print("超过尝试次数，放弃")
                    break
        if try_times > 4:
            continue

        try:
            print("开始解析第" + str(cate) + "类 第" + str(page_num) + "页")
            soup = BeautifulSoup(wbdata, "html.parser")
            links = soup.find_all("a", href=re.compile("/details?"), class_="", alt="")
            for link in links:
                 # 获取应用详情页面的链接
                try:
                    detail_link = urllib.parse.urljoin(_root_url, str(link["href"]))
                    package_name = detail_link.split("=")[1]
                    download_page = requests.get(detail_link,timeout=5).text
                    #解析应用详情页面
                    soup1 = BeautifulSoup(download_page, "html.parser")
                    download_link = soup1.find(class_="download")["href"]
                    #获取直接下载的链接
                    download_url = urllib.parse.urljoin(_root_url, str(download_link))
                    # 解析后会有重复的结果，通过判断去重
                    if download_url not in res_parser.values():
                        res_parser[package_name] = download_url
                except Exception as e:
                    print("解析下载页面错误：",e)
            print("解析完成：",len(res_parser))
        except Exception as e:
            print("解析应用页面错误",e)
            continue

        print("开始下载第" + str(cate) + "类 第" + str(page_num) + "页")
        for apk in res_parser.keys():
            print("正在下载应用: " + apk)
            # urllib.request.urlretrieve(res_dic[apk], save_path + apk + ".apk")
            try:
                flag = download(res_parser[apk],apk)
                if flag == True:
                    print("下载成功")
            except Exception as e:
                print(e)
        print("完成下载第" + str(cate) + "类 第" + str(page_num) + "页")

        dir_size = getdirsize(config.DATA_XIAOMI_APK_PATH)
        if dir_size > 200:
            print("下载容量超过200G，自动结束")
            sys.exit()


def getdirsize(dir):
    size = 0
    for root, dirs, files in os.walk(dir):
        size += sum([os.path.getsize(os.path.join(root, name)) for name in files])
    return size/1024/1024/1024 # Gb

def download(url,package_name):

    try_times = 0
    while True:
        try:
            res = utils.get_response(url,timeout=5)
            break
        except Exception as e:
            try_times += 1
            print("Error: ", e, " 第" + str(try_times) + "次重新尝试")
            if try_times > 3:
                print("超过尝试次数，放弃",url)
                return False

    file_size = int(res.headers['Content-Length'])/(1024**2)
    if file_size > 0 and file_size <= 20:
        file_path = os.path.join(config.DATA_XIAOMI_APK_PATH,url.split("/")[-1]+"_"+package_name + ".apk")
        if os.path.exists(file_path) == False:
            file = open(file_path, 'wb')
            block_size = 8192
            while True:
                buffer = res.read(block_size)
                if not buffer:
                    break
                file.write(buffer)
            file.close()
            return True
        else:
            print("文件已存在：",file_path,url)

        # buffer = res.read(size_limit*1024*1024)
        # md5 = hashlib.md5(buffer).hexdigest()
        # file_path = "./xiaomi/"+str(md5)+".apk"
        # if os.path.exists(file_path) == False:
        #     file = open(file_path,'wb')
        #     file.write(buffer)
        #     file.close()
        #     return True
        # else:
        #     print("文件已存在：",file_path,url)
    else:
        print("url:"+url+" size: "+str(file_size),", 链接为空或超过20M")
        return False


if __name__ == "__main__":
    #第10类下载完20-30
    cate_pages=[
        (5,67),
        (27,40),(2,67),
        (7,33),(12,67),
        (10,67),
        (9,67),(4,67),
        (3,67),(6,24),(14,67),
        (8,26),(11,57),
        (13,37),
        (1,67),
        (16,24),
        (17,55),(18,34),(19,43),(20,15),(21,10),(22,14),
        (23,67),(25,8),(26,19),(28,6),(29,52),(15,67)
    ]
    for cate, pages in cate_pages:
        parser_apks(cate,pages)




